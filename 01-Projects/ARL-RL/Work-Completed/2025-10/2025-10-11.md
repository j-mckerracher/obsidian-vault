--- 
project: ARL RL
tags: [project/arl-rl, work-log, slurm, bugfix]
date: 2025-10-11
---

# Daily Work Log — 2025-10-11

## FP16 masking overflow fix (autocast-safe)

### Summary
Fixed a mixed-precision (FP16) overflow in action masking during exploitation that caused the job to crash under torch.amp autocast.

Error observed:
```text
RuntimeError: value cannot be converted to type at::Half without overflow
  at training_split.py:447 in select_action
  q_masked[self.config.ACTION_NO_OP] = -1e9
```

### Root cause
- Under autocast('cuda'), tensors may be float16 (FP16).
- Using a constant like `-1e9` to penalize actions overflows FP16 (min finite ≈ -65504).

### Fix
- Use a dtype-safe large negative value for masking:
  - `neg_large = -1e4 if q_masked.dtype == torch.float16 else -1e9`
  - Build the mask with `torch.full_like(q_masked, neg_large)`
  - Assign `q_masked[self.config.ACTION_NO_OP] = neg_large` when heavily penalizing NO_OP

Patch (excerpt):
```python
# Before
q_masked = action_q.clone().squeeze(0)
mask = torch.ones_like(q_masked) * -1e9  # penalize unavailable actions
...
q_masked[self.config.ACTION_NO_OP] = -1e9

# After (autocast-safe)
q_masked = action_q.clone().squeeze(0)
neg_large = -1e4 if q_masked.dtype == torch.float16 else -1e9
mask = torch.full_like(q_masked, neg_large)
...
q_masked[self.config.ACTION_NO_OP] = neg_large
```

File changed:
- `training_split.py` (DQNAgent.select_action exploitation path)

### Related environment fixes
- SLURM job script now initializes Conda directly without loading any Lmod “anaconda” modules:
  - `source /depot/sbagchi/data/preeti/anaconda3/etc/profile.d/conda.sh` (or `eval "$($CONDA_ROOT/bin/conda shell.bash hook)"`) 
  - `conda activate /depot/sbagchi/data/preeti/anaconda3/envs/gpu`
- Avoids Lmod error "cannot load anaconda".

### Validation
- Job submitted and ran with autocast + FP16, past the action selection step without overflow.
- Logs showed:
  - Python: `/depot/.../envs/gpu/bin/python`
  - PyTorch: `2.7.0+cu126`, CUDA available: `True`
  - Environment ready; training started with Episode 1.
- Subsequent run (Job ID e.g., 9718771) proceeded without the prior exception.

### Next
- Monitor training curves and CSV aggregation (`e1_results.csv`).
- If masking strength needs tuning under FP16, adjust `neg_large` (e.g., -1e3 to -1e5) based on empirical behavior.
- Consider centralizing a utility for dtype-safe masking constants.

### Links
- [[2025-10-10 Stage E1 job submission success]]
- [[2025-10-10 Gilbreth partition-account-qos fix]]
- [[Submitting SLURM Jobs on Gilbreth for LLMs]]

---

## Stage E1 submission — normal QoS, 12h, 4k episodes

### Summary
Submitted Stage E1 training using normal QoS to allow a longer wall time window, and reduced per-seed episodes to 4,000 to ensure completion.

- Partition/account/QoS: a30 / sbagchi / normal
- Resources: ntasks=1, gpus=1 (via --gres), cpus-per-task=4, mem=50G, time=12:00:00
- Training: res=32×32, seeds=4 6 8, episodes=4,000 (per seed)
- Job ID: 9719090

### Commands
Wrapper invocation:
```bash
bash scripts/submit_e1_job.sh --account sbagchi --partition a30 --qos normal --time 12:00:00 --episodes 4000
```

Resulting sbatch:
```bash
sbatch --job-name=E1_Training_32x32 \
  --time=12:00:00 \
  --account=sbagchi \
  --partition=a30 \
  --qos=normal \
  --ntasks=1 \
  --gres=gpu:1 \
  --cpus-per-task=4 \
  --mem=50G \
  /home/jmckerra/Code/ARL-RL/scripts/run_e1.sh \
  --res 32 --seeds "4 6 8" --episodes 4000
```

Submission output:
```text
✅ Job submitted successfully!
Submitted batch job 9719090
```

### Rationale
- standby QoS window is too short for 10k episodes across 3 seeds.
- normal QoS allows longer wall times (subject to account policy), and reducing episodes per job increases the chance of completion.

### Notes
- If needed, further split runs by seed or chunk episodes per job (see Engineering Notes for chunking and job dependencies).

### Monitoring
```bash
squeue -j 9719090
cat e1_training_9719090.out
cat e1_training_9719090.err
```

