---
title: "Jobs for expt-20251203-e2-res64 — E2 Resolution 64x64"
date: "2025-12-03"
last_updated: "2025-12-03T10:00:00Z"
tags: ["project/arl-rl", "job"]
experiment: "[[Documents/Experiments/expt-20251203-e2-res64]]"
scheduler: "slurm"
cluster: "Gilbreth HPC"
account: "sbagchi"
resources:
  nodes: 1
  gpus: 1
  gpu_model: "A30"
  cpus: 4
  mem_gb: 80
  time: "04:00:00"
  partition: "a30"
  qos: "standby"
env_setup:
  conda_env: "/depot/sbagchi/data/preeti/anaconda3/envs/gpu"
workdir: "/home/jmckerra/ARL-RL"
job_ids: []
status: "submitted"
---

# Job Submission — E2 Resolution Scaling (64x64)

## Overview
Testing the frozen E2 configuration with 64x64 resolution.
Using **Standby QoS** and **80GB Memory** as per [[Don't Forget]] recommendations to avoid OOM and queue delays.

## Commands

### Single Batch Submission (Seeds 4, 6, 8)
We submit one array-like job (handled by the script's internal loop) or a single script execution that runs the seeds sequentially/parallel depending on script logic. The provided `run_e2.sh` iterates through seeds.

**Command to run on cluster:**
```bash
cd /home/jmckerra/Code/ARL-RL

sbatch \
  --account sbagchi \
  --partition a30 \
  --qos standby \
  --time 4:00:00 \
  --mem 80G \
  scripts/run_e2.sh \
  --res 64 \
  --seeds "4 6 8" \
  --episodes 500
```

*Note: `scripts/run_e2.sh` handles the loop over seeds 4, 6, and 8 sequentially for this smoke test.*

## Notes
- **Why Standby?** To bypass `AssocGrpGRES` limits and get scheduled faster.
- **Why 80GB?** 64x64 input is 4x larger than 32x32.
- **Why 500 Episodes?** Initial smoke test to verify stability and initial learning curve before committing to a full 3k run.

## Outputs & Logs
- **Results CSV**: `/depot/sbagchi/data/josh/RL/FindAndDefeatZerglings/results_split_advanced/e2_results.csv`
- **Run Logs**: `e2_training_<JOBID>.out` / `.err`

## Job IDs
- Job ID: (To be filled after submission)

## Changelog
- 2025-12-03T10:00:00Z Created from Job.template.md
