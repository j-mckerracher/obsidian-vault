@article{amvrosiadis2018atlas,
  title={The Atlas cluster trace repository},
  author={Amvrosiadis, George and Kuchnik, Michael and Park, Jun Woo and Cranor, Chuck and Ganger, Gregory R and Moore, Elisabeth and DeBardeleben, Nathan},
  journal={Usenix Mag},
  volume={43},
  number={4},
  pages={29--35},
  year={2018}
}

@misc{cfdr,
  author = {Bianca Schroeder and Garth Gibson},
  title = {The Computer Failure Data Repository (CFDR)},
  url = {https://www.usenix.org/cfdr},
  year = {2015}
}

@misc{lanl_failure_data,
  author = {{Los Alamos National Laboratory}},
  title = {Failure Data},
  howpublished = {U.S. Reliability and Resilience Characterization (USRC)},
  url = {https://usrc.lanl.gov/data%20sources/failure-data.php},
}

@inproceedings{mckerracher2025fresco,
  author    = {McKerracher, Joshua Stephen and Mukherjee, Preeti and Kalyanam, Rajesh and Bagchi, Saurabh},
  title     = {FRESCO: A Public Multi-Institutional Dataset for Understanding HPC System Behavior and Dependability},
  booktitle = {Practice and Experience in Advanced Research Computing (PEARC '25)},
  year      = {2025},
  month     = {jul},
  publisher = {Association for Computing Machinery (ACM)},
  address   = {New York, NY, USA},
  articleno = {111},
  numpages  = {6},
  doi       = {10.1145/3708035.3736090},
  url       = {https://doi.org/10.1145/3708035.3736090}
}


%{reddit_slurm_sucked,
  %author       = "{Ok_Post_149}",
  %title        = {slurm sucked for me as an end user. that's why I'm fixing it},
  %year         = {2024},
  %month        = {feb},
  %howpublished = {Reddit Post},
 % url          = {https://www.reddit.com/r/HPC/comments/1ieqt4c/slurm_sucked_for_me_as_an_end_user_thats_why_im/},
%  note         = {Accessed: 2025-08-14}
%}

@misc{reddit_slurm_slower,
  author       = "{librapenseur}",
  title        = {{SLURM} jobs running much slower under most circumstances},
  year         = {2024},
  month        = {feb},
  howpublished = {Reddit Post},
  url          = {https://www.reddit.com/r/HPC/comments/1axvogm/slurm_jobs_running_much_slower_under_most/},
  note         = {Accessed: 2025-08-14}
}

@misc{reddit_hpc_etiquette,
  author       = "{yourradio}",
  title        = {{HPC} usage etiquette},
  year         = {2023},
  month        = {may},
  howpublished = {Reddit Post},
  url          = {https://www.reddit.com/r/HPC/comments/13dm0xt/hpc_usage_etiquette/},
  note         = {Accessed: 2025-08-14}
}

@inproceedings{Li_Michelogiannakis_Cook_Cooray_Chen_2023,
  author    = {Jie Li and George Michelogiannakis and Brandon Cook and Dulanya Cooray and Yong Chen},
  title     = {Analyzing Resource Utilization in an HPC System: A Case Study of NERSC Perlmutter},
  booktitle = {International Supercomputing Conference (ISC)},
  series    = {Lecture Notes in Computer Science},
  volume    = {13948},
  pages     = {297--316},
  year      = {2023},
  doi       = {10.1007/978-3-031-32041-5_16},
}

@misc{google2011trace,
  author = {{Google Research Blog}},
  title = {Yet More Google Compute Cluster Trace Data},
  howpublished = {\url{https://research.google/blog/yet-more-google-compute-cluster-trace-data/}},
  year = {2020},
  note = {Accessed: 2025-08-15}
}

@inproceedings{amvrosiadis2017representative,
  author    = {Amvrosiadis, George and Beck, Christopher and Ganger, Gregory R. and Moore, Elisabeth and Park, Jun Woo and Cranor, Chuck and Kuchnik, Michael},
  title     = {What Do Cluster Jobs Look Like Outside Google?},
  booktitle = {Proceedings of the 8th ACM European Conference on Computer Systems (EuroSys ’17)},
  year      = {2017},
  pages     = {354–369},
  doi       = {10.1145/3064176.3064210}
}

@techreport{10.5555/893785,
author = {Downey, Allen B.},
title = {A Parallel Workload Model and its Implications for Processor},
year = {1996},
publisher = {University of California at Berkeley},
address = {USA},
abstract = {We develop a workload model based on the observed behavior of parallel computers at the San Diego Supercomputer Center and the Cornell Theory Center. This model gives us insight into the performance of strategies for scheduling malleable jobs on space-sharing parallel computers. We find that Adaptive Static Partitioning (ASP), which has been reported to work well for other workloads, is inferior to some FIFO strategies that adapt better to system load. The best of the strategies we consider is one that explicitly restricts cluster sizes when load is high (a variation of Sevcik''s A+ strategy [Sevcik89].}
}

@InProceedings{10.1007/BFb0053984,
author="Smith, Warren
and Foster, Ian
and Taylor, Valerie",
editor="Feitelson, Dror G.
and Rudolph, Larry",
title="Predicting application run times using historical information",
booktitle="Job Scheduling Strategies for Parallel Processing",
year="1998",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="122--142",
abstract="We present a technique for deriving predictions for the run times of parallel applications from the run times of ``similar'' applications that have executed in the past. The novel aspect of our work is the use of search techniques to determine those application characteristics that yield the best definition of similarity for the purpose of making predictions. We use four workloads recorded from parallel computers at Argonne National Laboratory, the Cornell Theory Center, and the San Diego Supercomputer Center to evaluate the effectiveness of our approach. We show that on these workloads our techniques achieve predictions that are between 14 and 60 percent better than those achieved by other researchers; our approach achieves mean prediction errors that are between 40 and 59 percent of mean application run times.",
isbn="978-3-540-68536-4"
}

@article{VERCELLINO2023215,
title = {A Machine Learning Approach for an HPC Use Case: the Jobs Queuing Time Prediction},
journal = {Future Generation Computer Systems},
volume = {143},
pages = {215-230},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23000274},
author = {Chiara Vercellino and Alberto Scionti and Giuseppe Varavallo and Paolo Viviani and Giacomo Vitali and Olivier Terzo},
keywords = {High performance computing, Queues, Batch scheduler, Automatism, Machine learning, Uncertainty quantification},
abstract = {High-Performance Computing (HPC) domain provided the necessary tools to support the scientific and industrial advancements we all have seen during the last decades. HPC is a broad domain targeting to provide both software and hardware solutions as well as envisioning methodologies that allow achieving goals of interest, such as system performance and energy efficiency. In this context, supercomputers have been the vehicle for developing and testing the most advanced technologies since their first appearance. Unlike cloud computing resources that are provided to the end-users in an on-demand fashion in the form of virtualized resources (i.e., virtual machines and containers), supercomputers’ resources are generally served through State-of-the-Art batch schedulers (e.g., SLURM, PBS, LSF, HTCondor). As such, the users submit their computational jobs to the system, which manages their execution with the support of queues. In this regard, predicting the behaviour of the jobs in the batch scheduler queues becomes worth it. Indeed, there are many cases where a deeper knowledge of the time experienced by a job in a queue (e.g., the submission of check-pointed jobs or the submission of jobs with execution dependencies) allows exploring more effective workflow orchestration policies. In this work, we focused on applying machine learning (ML) techniques to learn from the historical data collected from the queuing system of real supercomputers, aiming at predicting the time spent on a queue by a given job. Specifically, we applied both unsupervised learning (UL) and supervised learning (SL) techniques to define the most effective features for the prediction task and the actual prediction of the queue waiting time. For this purpose, two approaches have been explored: on one side, the prediction of ranges on jobs’ queuing times (classification approach) and, on the other side, the prediction of the waiting time at the minutes level (regression approach). Experimental results highlight the strong relationship between the SL models’ performances and the way the dataset is split. At the end of the prediction step, we present the uncertainty quantification approach, i.e., a tool to associate the predictions with reliability metrics, based on variance estimation.}
}

@article{brown2022predicting,
  title={Predicting batch queue job wait times for informed scheduling of urgent HPC workloads},
  author={Brown, Nick and Gibb, Gordon and Belikov, Evgenij and Nash, Rupert},
  journal={arXiv preprint arXiv:2204.13543},
  year={2022},
  url={https://arxiv.org/abs/2204.13543}
}
\@Article{feitelson14-pwaexp,
author	=	{Dror G. Feitelson and Dan Tsafrir and David Krakov},
title	=	{Experience with using the {Parallel} {Workloads} {Archive}},
journal	=	{Journal of Parallel and Distributed Computing (JPDC)},
volume	=	74,
number	=	10,
year	=	2014,
month	=	{October},
pages	=	{2967--2982}
}

@inproceedings{10.1109/HUST.2014.7,
author = {Evans, Todd and Barth, William L. and Browne, James C. and DeLeon, Robert L. and Furlani, Thomas R. and Gallo, Steven M. and Jones, Matthew D. and Patra, Abani K.},
title = {Comprehensive resource use monitoring for HPC systems with TACC stats},
year = {2014},
isbn = {9781467367554},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/HUST.2014.7},
doi = {10.1109/HUST.2014.7},
abstract = {This paper reports on a comprehensive, fully automated resource use monitoring package, TACC Stats, which enables both consultants, users and other stakeholders in an HPC system to systematically and actively identify jobs/applications that could benefit from expert support and to aid in the diagnosis of software and hardware issues. TACC Stats continuously collects and analyzes resource usage data for every job run on a system and differs significantly from conventional profilers because it requires no action on the part of the user or consultants---it is always collecting data on every node for every job. TACC Stats is open source and downloadable, configurable and compatible with general Linux-based computing platforms, and extensible to new CPU architectures and hardware devices. It is meant to provide a comprehensive resource usage monitoring solution. In addition to describing TACC Stats, the paper illustrates its application to identifying production jobs which have inefficient resource use characteristics.},
booktitle = {Proceedings of the First International Workshop on HPC User Support Tools},
pages = {13–21},
numpages = {9},
location = {New Orleans, Louisiana},
series = {HUST '14}
}
@inproceedings{10.1145/3488423.3519336,
author = {Peng, Ivy and Karlin, Ian and Gokhale, Maya and Shoga, Kathleen and Legendre, Matthew and Gamblin, Todd},
title = {A Holistic View of Memory Utilization on HPC Systems: Current and Future Trends},
year = {2022},
isbn = {9781450385701},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488423.3519336},
doi = {10.1145/3488423.3519336},
abstract = {Memory subsystem is one crucial component of a computing system. Co-designing memory subsystems becomes increasingly challenging as workloads continue evolving on HPC facilities and new architectural options emerge. This work provides the first large-scale study of memory utilization with system-level, job-level, temporal and spatial patterns on a CPU-only and a GPU-accelerated leadership supercomputer. From system-level monitoring data that spans three years, we identify a continuous increase in memory intensity in workloads over recent years. Our job-level characterization reveals different hotspots in memory usage on the two systems. Furthermore, we introduce two metrics, ’spatial imbalance’ and ’temporal imbalance’, to quantify the imbalanced memory usage across compute nodes and throughout time in jobs. We identify representative temporal and spatial patterns from real jobs, providing quantitative guidance for research on efficient resource configurations and novel architectural options. Finally, we showcase the impact of our study in informing system configurations through an upcoming NNSA CTS procurement.},
booktitle = {Proceedings of the International Symposium on Memory Systems},
articleno = {14},
numpages = {11},
keywords = {HPC, large-scale characterization, memory characterization, memory systems, system-wide characterization},
location = {Washington DC, DC, USA},
series = {MEMSYS '21}
}

@misc{mckerracher2025hpc_transformers,
  author       = {Joshua Stephen McKerracher},
  title        = {hpc\_transformers.py (Lines 122–219)},
  year         = {2025},
  howpublished = {\url{https://github.com/j-mckerracher/fresco-hpc/blob/main/data-pipeline/hpc_etl_pipeline/src/transformers/hpc_transformers.py#L122-L219}},
  note         = {Accessed: 2025-08-16}
}

@misc{fresco-analyses-github,
  title   = {FRESCO HPC Analyses: COMAD workflows},
  author  = {McKerracher, Joshua},
  year    = {2025},
  howpublished = {\url{https://github.com/j-mckerracher/fresco-hpc-analyses/tree/main/comad}},
  note    = {Accessed: 2025-08-16; tag 1.0.0, commit 0d6068f}
}
